{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graphs\n",
    "\n",
    "Computational graphs (CG) are a way of representing mathematical functions and how values propagate between mathematical expressions. Among other things, the study of CG leads to deeper understanding of calculus, especially in simplifying taking derivatives by breaking complex expressions into simpler ones and chaining them up. When implemented in a programming language, CGs are able to automatically differentiate functions numerically exact or even symbolically. \n",
    "\n",
    "CGs thus play an important rule in mathematical function optimization, especially when computing derivatives is analytically infeasable. Supervised training of neural networks, for example, maps to optimizing (minimizing) a cost function with respect to all the neural weights in the network chained in different layers. Optimizing is usually performed using a gradient descent approach which requires first order derivatives of the cost function with respect to all the parameters in the network. As we will see, CGs not only make this doable but also provide an computationally efficient algorithm named [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) to compute the partial derivatives.\n",
    "\n",
    "## What this is about\n",
    "\n",
    "This notebook gives an introduction into CGs and how to implement them in Python. By the end of the notebook we will have a working framework that\n",
    " - is able to evaluate mathematical expressions represented as CGs,\n",
    " - perform [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) to find numerically exact  derivatives (up to floating point precision),\n",
    " - perform [symbolic differentiation](https://en.wikipedia.org/wiki/Symbolic_computation) to deduce higher order derivatives,\n",
    " - simplify expressions to improve performance and readability.\n",
    "\n",
    "## What it isn't about\n",
    "\n",
    "To keep the code basis readable there are some shortcomings to the developed framework. Foremost it is not complete. That means you won't be able to plugin every possible function and expect it return the correct result. This is mostly a problem of not providing derivatives for all elementary functions. However, the framework is structured in such a way that you will find it easy to add new blocks it and make it even more feature complete.\n",
    "\n",
    "Also, we'll be mostly dealing with so called multi variate real-valued scalar functions $f:\\mathbb {R} ^{n}\\to \\mathbb {R}$. In other words, we constrain our self to real values and functions that have multiple inputs but only output a single scalar. A glimpse on how to use the framework for vector-valued functions will be given in the examples section towards the end of this document.\n",
    "\n",
    "## Introduction to computational graphs (CG)\n",
    "\n",
    "Consider a function $$f(x,y) := (x + y)x$$\n",
    "\n",
    "We'll call $x$ and $y$ symbols, $+$ and $*$ will be called operations / functions / nodes. When we evaluate $f$, we first add up the values of $x$ and $y$ and then multiply the result by $x$. The output of the multiplication is what we call the value of $f(x,y)$. Now, a CG is a [directed graph](https://en.wikipedia.org/wiki/Directed_graph) that represents this procedure. A graphical representation of CG for $f$ is shown below\n",
    "\n",
    "<img src=\"intro_0.png\" width=\"400\">\n",
    "\n",
    "### Computing the value of $f(x,y)$\n",
    "\n",
    "Computing the value of $f$ in the CG is a matter of following directed edges of the graph. Assume we want to evaluate $f(2,3)$. First, we send 2 and 3 along the out-edges of $x$ and $y$ respectively.\n",
    "\n",
    "<img src=\"intro_1.png\" width=\"400\">\n",
    "\n",
    "Next, we compute the value of $+$. Note that $*$ cannot be computed as one of its inputs, namely $+$ is missing.\n",
    "\n",
    "<img src=\"intro_2.png\" width=\"400\">\n",
    "\n",
    "Finally, we find value of $f$ by evaluating $*$\n",
    "\n",
    "<img src=\"intro_3.png\" width=\"400\">\n",
    "\n",
    "In evaluating the value of $f$ one needs to process all processors of a node $n$ before evaluating $n$ itself. Such an ordering on a CG is called a [topological order](https://en.wikipedia.org/wiki/Topological_sorting). \n",
    "\n",
    "### Computing the partial derivatives\n",
    "\n",
    "We will now turn our attention towards computing of [partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative). In order to do so, we will be a bit more abstract and use symbols for the outputs of all nodes. Like so.\n",
    "\n",
    "<img src=\"intro_4.png\" width=\"400\">\n",
    "\n",
    "Please take a moment and convince yourself that $f(x,a) = f(x,y)$. \n",
    "\n",
    "#### Partial derivatives of isolated nodes\n",
    "\n",
    "Now consider any node in this CG in isolation, irrespectively of where it is located in the graph. When computing partial derivatives of isolated nodes with respect to their inputs, each input is treated as a separate abstract symbol. Therefore taking the derivative completely ignores the fact that each input symbol might be the result a complex operation by itself. By taking this perspective of isolating nodes, computation of derivatives can be simplified.\n",
    "\n",
    "For example, take the multiplication node \n",
    "\n",
    "$$f(x,a) := ax$$ \n",
    "\n",
    "in isolation. Computing the partial derivatives of $f(x,a)$ with respect to its inputs, requires\n",
    "\n",
    "$$\\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}a}, \\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}x}$$\n",
    "\n",
    "to be found. Of course this will amount to\n",
    "\n",
    "$$\\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}a}=x, \\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}x}=a$$\n",
    "\n",
    "Similarily, for the addition node\n",
    "\n",
    "$$a(x,y) := x+y$$\n",
    "\n",
    "we get\n",
    "\n",
    "$$\\frac{\\mathrm{d}a(x,y)}{\\mathrm{d}x}=1, \\frac{\\mathrm{d}a(x,y)}{\\mathrm{d}y}=1$$\n",
    "\n",
    "In order to display the partial derivatives in the CG diagrams, we will use backward oriented arrows between nodes to which we attach the corresponding partial derivatives as shown below.\n",
    "\n",
    "<img src=\"intro_5.png\" width=\"400\">\n",
    "\n",
    "For each node, we consider all of its inputs. For each input arrow we generate a corresponding backward oriented arrow and assign it the partial derivative of the node's output with respect to the input arrow. Note that if a node has multiple inputs and a single output, we call the partial derivatives with respect to all inputs the [gradient](https://en.wikipedia.org/wiki/Gradient).\n",
    "\n",
    "#### Partial derivatives of nodes with respect to other nodes\n",
    "\n",
    "To compute the partial derivative for any node $n$ with respect to function $f(x,y):=(x+y)x$, or $f(x,a)$ in the CG diagram, one needs to\n",
    " 1. Find all the backward pathes starting from $f(x,a)$ reaching $n$.\n",
    " 1. For each path build the product of partial derivatives along its chain of backward arrows.\n",
    " 1. Sum over all path products of partial derivatives.\n",
    " \n",
    "For example, the partial derivative of $\\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}y}$ amounts to\n",
    "\n",
    "$$\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}y} = \n",
    "\\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}f(x,a)}\n",
    "\\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}a(x,y)}\n",
    "\\frac{\\mathrm{d}a(x,y)}{\\mathrm{d}y}\n",
    "$$\n",
    "\n",
    "There is only one backward oriented path from $f(x,a)$ to $y$, so there is no summation. Note that $f(x,a)$ is equivalent to $f(x,y)$ by construction. By substitution (see CG diagram above) we find\n",
    "\n",
    "$$\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}y} = 1*x*1 = x$$\n",
    "\n",
    "Finding $\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}x}$ is very similar, except that this time we have two paths in which $x$ can be reached from $f(x,a)$.\n",
    "\n",
    "$$\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}x} = \n",
    "    \\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}f(x,a)}\n",
    "    \\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}a(x,y)}\n",
    "    \\frac{\\mathrm{d}a(x,y)}{\\mathrm{d}x} +\n",
    "    \\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}f(x,a)}\n",
    "    \\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}x}   \n",
    "$$\n",
    "\n",
    "By subsitution we get\n",
    "\n",
    "$$\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}x} = 1*x*1 + 1*a(x,y) = x + (x+y) = 2x+y$$\n",
    "\n",
    "With these tools you are able to compute the derivatives of 'any' function, no matter how complex it is. Here's what you do:\n",
    "First, decomposing the function into operations simple enough, so you know how to compute isolated partial derivatives for individual nodes. Next, build a CG based on your decomposition. Apply the 3-step algorithm above to find partial derivatives of any two connected nodes in the CG (usually you do this for the input variables).\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "While you can use the above recipe to compute partial derivatives for any two nodes connected by a path in the CG, it should be noted that it is not computationally efficient yet. Notice the terms \n",
    "\n",
    "$$\\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}f(x,a)}\\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}a(x,y)}$$\n",
    "\n",
    "appear in computations of both $\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}x}$ and $\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}y}$. A more clever way to spare redundant computations is this: Turn the backward arrows into a computational graph on its own and apply a topological sorting to it. This is the same procedure as we've seen in evaluting a CG, but this time the order is reversed (as backward edges are used). Traverse the CG, starting from $f(x,a)$ in order and perform the following for each node $n$.\n",
    " 1. Compute $\\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}n}$ by summing over all incoming backward edges (backward arrows are now node inputs)\n",
    " 1. Compute the isolated partial derivatives with respect to forward incoming edges $i$ as $\\frac{\\mathrm{d}n}{\\mathrm{d}n_i}$\n",
    " 1. For each outgoing backward edge compute the product $\\frac{\\mathrm{d}f(x,a)}{\\mathrm{d}n}\\frac{\\mathrm{d}n}{\\mathrm{d}n_i}$.\n",
    "\n",
    "The above procedure in pictures looks like this. First, we let the value along the very first backward edge be 1. This will allow us to kickstart the procedure above.\n",
    "\n",
    "<img src=\"intro_6.png\" width=\"400\">\n",
    "\n",
    "Next we process $f(x,a)$\n",
    "\n",
    "<img src=\"intro_7.png\" width=\"400\">\n",
    "\n",
    "Then we process $a(x,y)$\n",
    "\n",
    "<img src=\"intro_8.png\" width=\"400\">\n",
    "\n",
    "Finally at $x$ and $y$ we would only perform the first step of the above algorithm to correctly compute $\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}x}$ and $\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}y}$\n",
    "\n",
    "When computing the partial derivatives numerically, one would evaluate the terms at the corresponding values that propagated through the graph in the forward (value) pass. Reconsidering our numeric example from before, were we let $x=2$ and $y=3$ we'd get the following graph\n",
    "\n",
    "<img src=\"intro_9.png\" width=\"400\">\n",
    "\n",
    "Finally at $x$ and $y$ we'd get $\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}x}=7$ and $\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}y}=2$. Mind you that we also know the partial derivatives with respect to $f(x,y)$ for all the intermediate nodes (just the addition in this example). All it took was visiting each node twice, once when computing the node's output value and once in the backward pass.\n",
    "\n",
    "The chained forward-backward passes as explained above is also known as reverse [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), a technique to numerically evaluate the partial derivatives of a function composed of a set of operations for which isolated partial derivatives are known. Note, auto-diff can be executed also in forward mode which won't be covered in this text.\n",
    "\n",
    "Note that backpropagation in neural network training refers not only to backpropagating partial derivatives of a loss function with respect to neural weights and biases, but also means application of gradient descent methods to update the parameters accordingly, so that the objective function is mininmized. We will come back to that when doing function optimization.\n",
    "\n",
    "### Summary and Outlook\n",
    "\n",
    "Up to this point computational graphs (CG) were introduced. A method to compute the value of a CG was presented that used forward propagation of values. Next, a recipe to compute the partial derivatives through backward traversal was given. We've improved the initial formulation by factoring out common terms in what lead to the backpropagation algorithm.\n",
    "\n",
    "## Expression trees\n",
    "\n",
    "Before diving into code, we need to cover the concept of [expression trees](https://en.wikipedia.org/wiki/Binary_expression_tree). Expression trees will be used to represent function decompositions in Python code to be developed. While there are not a fundamentally new concept they deserve some words at this point.\n",
    "\n",
    "An expression tree is similar to the computational graphs introduced, but the arrows by default point backwards. It turns out that constructing function expression in a tree like manner (top node is the function itself, function parameters are leafs) simplifies development dramatically.\n",
    "\n",
    "Take the CG of the toy example used $f(x,y)=(x+y)x$\n",
    "\n",
    "<img src=\"intro_0.png\" width=\"400\">\n",
    "\n",
    "The following expression tree represents the same function\n",
    "\n",
    "<img src=\"exp_tree.png\" width=\"400\">\n",
    "\n",
    "Notice that we now have a tree like structure. Our root node is the final operation to be executed to receive the result of $f(x,y)$. Also notice that $x$ shows up twice. Finding the value of an expression tree requires to compute values for nodes in lower layers first and bubble information up towards the root node. Backpropagation on the other hand ist just a matter of following the forward edges. Again, when computing derivatives, a summation over all paths from the top that lead to a given node will be performed.\n",
    "\n",
    "## CGraph \n",
    "\n",
    "CGraph is the name of the Python library to be developed during the remainder of this notebook. While the code inside the notebook is functional a separate, self-contained and enhanced implementation of CGraph is available [cgraph.py](../cgraph.py). \n",
    "\n",
    "CGraph performs numeric and symbolic differentiation using backpropagation.\n",
    "\n",
    "```Python\n",
    "import cgraph as cg\n",
    "\n",
    "x = cg.Symbol('x')\n",
    "y = cg.Symbol('y')\n",
    "z = cg.Symbol('z')\n",
    "\n",
    "f = (x * y + 3) / (z - 2)\n",
    "\n",
    "# Evaluate function\n",
    "cg.value(f, {x:2, y:3, z:3}) # 9.0\n",
    "\n",
    "# Partial derivatives (numerically)\n",
    "d = cg.numeric_gradient(f, {x:2, y:3, z:3})\n",
    "d[x] # df/dx 3.0\n",
    "d[z] # df/dz -9.0\n",
    "\n",
    "# Partial derivatives (symbolically)\n",
    "d = cg.symbolic_gradient(f)\n",
    "cg.simplify(d[x]) # (y*(1/(z - 2)))\n",
    "cg.value(d[x], {x:2, y:3, z:3}) # 3.0\n",
    "\n",
    "# Higher order derivatives\n",
    "ddx = cg.symbolic_gradient(d[x])\n",
    "cg.simplify(ddx[y]) # ddf/dxdy\n",
    "# (1/(z - 2))\n",
    "```\n",
    "\n",
    "Python 3.5 will be used for development. The reader is assumed to be familiar with its concepts including generators and decorators. Also a technique called monkey patching will be used to iteratively refine classes already introduced.\n",
    "\n",
    "### Representing expression trees\n",
    "First we need to come up with a way to represent expression trees in Python code. Naturally, we will have base class `Node` that manages child references and derived classes that actually implement operations, symbols and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self, nary=0):\n",
    "        self.children = [None]*nary\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Node` for now just tracks references to its children. Note that operations can be binary (e.g. addition), unary (e.g cosine) or don't have children at all (e.g. symbols). We can also think of n-ary functions such as summation. Next, we'll define the leaf nodes `Symbol` and `Constant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Symbol(Node):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        super(Symbol, self).__init__(nary=0)\n",
    "        self.name = name\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.name)            \n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.name == other.name      \n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symbols are identified by their name, like $x$, and they don't have any children. When printed we print the name of the symbol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Constant(Node):\n",
    "\n",
    "    def __init__(self, value):\n",
    "        super(Constant, self).__init__(nary=0)\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Constants` are 'immutable' values. Next we start to add operations. For this notebook we will provide addition, multiplication. [cgraph.py](../cgraph.py) has more operations defined and once you know how to implement them it will be easy for you to add new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Add, self).__init__(nary=2)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '({}+{})'.format(str(self.children[0]), str(self.children[1]))\n",
    "    \n",
    "class Mul(Node):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Mul, self).__init__(nary=2)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '({}*{})'.format(str(self.children[0]), str(self.children[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Add` and `Mul` don't do much yet expect that stating that they are binary functions plus some pretty printing (recursively calling `__str__` on its children). Next, well just have a helper function that builds our toy function $f(x,y)=(x+y)x$. This looks a bit clumsy right now but we'll improve the syntax as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((x + y)*x)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_f(x, y): \n",
    "    a = Add()\n",
    "    a.children[0] = x\n",
    "    a.children[1] = y\n",
    "    \n",
    "    m = Mul()\n",
    "    m.children[0] = a\n",
    "    m.children[1] = x\n",
    "    \n",
    "    return m\n",
    "\n",
    "x = Symbol('x')\n",
    "y = Symbol('y')\n",
    "f = gen_f(x, y)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing function values\n",
    "\n",
    "Next we'll turn our attention towards computing values of functions represented as expression trees. As mentioned earlier to compute the value, we'll need to bubble up information from layers further down in hierarchy up to the root. Traversing expression trees can be performed in multiple ways. What we are looking for is [depth-first-search](https://en.wikipedia.org/wiki/Depth-first_search) in [post-order](https://en.wikipedia.org/wiki/Tree_traversal). There are many ways to implement the traversal, i've chosen the recursive generator approach because of its shortness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postorder(node):\n",
    "    for c in node.children:\n",
    "        yield from postorder(c)\n",
    "    yield node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x, y, (x + y), x, ((x + y)*x)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in postorder(f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, children are evaluated before their parents. Excactly what's needed for computing function values. Next, define the method that computes the forward pass, i.e the value of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def values(f, fargs):\n",
    "    \"\"\"Returns a dictionary of computed values for each node in the expression tree including `f`.\"\"\"\n",
    "    v = {}\n",
    "    v.update(fargs)\n",
    "    for n in postorder(f):\n",
    "        if not n in v:\n",
    "            v[n] = n.compute_value(v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method calls for each node `compute_value(values)` and expects the node to return its value. Since we haven't defined this function for our nodes yet, it's time to do so. Also note that `fargs` will be assumed to contain the values for the symbols in the expression tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Monkey patching for compute_value\n",
    "Symbol.compute_value = lambda self, values: values[self]\n",
    "Constant.compute_value = lambda self, values : self.value\n",
    "Add.compute_value = lambda self, values: values[self.children[0]] + values[self.children[1]]\n",
    "Mul.compute_value = lambda self, values: values[self.children[0]] * values[self.children[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After monkey patching in `compute_value` for all nodes we can evaluate `f` by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values(f, {x:2, y:3})[f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `values` computes all the values even for intermediate nodes we need to add `[f]` as a postfix. Just accessing the value of `f` will however be so common task that we provide a shortcut for it named `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value(f, fargs):\n",
    "    return values(f, fargs)[f]\n",
    "\n",
    "value(f, {x:2, y:3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic sugar\n",
    "\n",
    "Before continuing it makes sense to use Python's internal methods for 'overloading' the `+` and `*` operation for Nodes. First, we'll define a decorator that will wrap plain numbers to `Constants`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "\n",
    "def wrap_args(func):\n",
    "    \"\"\"Wraps function arguments that are numbers as Constant objects.\"\"\"\n",
    "    def wrapped(*args, **kwargs):\n",
    "        new_args = []\n",
    "        for a in args:\n",
    "            if isinstance(a, Number):\n",
    "                a = Constant(a)\n",
    "            new_args.append(a)\n",
    "        return func(*new_args, **kwargs)\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define some free functions that perform the 'lengthy' addition and multiplication. By convention these free functions will start with the prefix `sym_` (for symbolic). When adding new functionality you should always provide such a function (e.g `sym_pow, sym_cos`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@wrap_args\n",
    "def sym_add(x, y):\n",
    "    n = Add()\n",
    "    n.children[0] = x\n",
    "    n.children[1] = y\n",
    "    return n\n",
    "\n",
    "@wrap_args\n",
    "def sym_mul(x, y):\n",
    "    n = Mul()\n",
    "    n.children[0] = x\n",
    "    n.children[1] = y\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we monkey patch `Node` to support `+` and `*` operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Node.__add__ = lambda self, other: sym_add(self, other)\n",
    "Node.__radd__ = lambda self, other: sym_add(other, self)\n",
    "Node.__mul__ = lambda self, other: sym_mul(self, other)\n",
    "Node.__rmul__ = lambda self, other: sym_mul(other, self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `__r*` methods are also overloaded so that expressions of the type `n*3` and `3*n` work equally well. With that we can rewrite `gen_f` introduced by simply saying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = (x + y)*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing numeric derivatives\n",
    "\n",
    "Next we will turn our attention to the backpropagation for computing numerical derivatives. For this we first need another traversal, one that visits all nodes on the same level before moving on to the next level. Such a traversal is called [breadth-first-search](https://en.wikipedia.org/wiki/Breadth-first_search) and it can also be implemented in numerous ways.\n",
    "\n",
    "The way it is implemented here is based on a generator that uses a queue. However when performing backpropagation, we'd like to communicate values back to the generator for each children of the current node. We then expect the handed values to be passed to us when we visit the corresponding child. Doing so turns the generator into [co-routine](https://en.wikipedia.org/wiki/Coroutine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bfs(node, node_data):\n",
    "    q = [(node, node_data)]\n",
    "    while q:\n",
    "        t = q.pop(0)\n",
    "        node_data = yield t\n",
    "        for idx, c in enumerate(t[0].children):\n",
    "            q.append((c, node_data[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, `numeric_gradient` is presented. It takes an expression tree and function arguments for the contained symbols and computes all numeric partial derivatives with respect to the root node passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def numeric_gradient(f, fargs):\n",
    "    vals = values(f, fargs)\n",
    "    derivatives = defaultdict(int) # by default 0 is the derivative for unknown nodes.\n",
    "\n",
    "    gen = bfs(f, 1)\n",
    "    try:\n",
    "        n, in_grad = next(gen)\n",
    "        while True:\n",
    "            derivatives[n] += in_grad\n",
    "            local_grad = n.compute_gradient(vals)\n",
    "            n, in_grad = gen.send([l*in_grad for l in local_grad])\n",
    "    except StopIteration:\n",
    "        return derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, numeric_gradient performs a forward pass to compute all function values. Next, breadth-first-search is kicked of by f and a value of 1. Then for each node visited we accumulate incoming partial derivatives send along the edges. Next, the local isolated gradient is computed. We communicate back the local gradient times incoming partial derivative as explained in the backpropagation introduction before. Finally a dictionary of partial derivatives for each operation is returned.\n",
    "\n",
    "Also note that we need to provide implementations of `compute_gradient(values)` for each node. `compute_gradient` is expected to take in a values dictionary and return the isolated partial derivative for every children in array form. As always, lets monkey patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Monkey patch for compute_gradient\n",
    "Symbol.compute_gradient = lambda self, values: [] # Nothing todo\n",
    "Constant.compute_gradient = lambda self, values: [] # Nothing todo\n",
    "\n",
    "Add.compute_gradient = lambda self, values: [1, 1] # dx+y/dx = 1, dx+y/dy = 1\n",
    "Mul.compute_gradient = lambda self, values: [values[self.children[1]], values[self.children[0]]] # dx*y/dx = y, dx*y/dy = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The isolated gradients for `Add` and `Mul` should look familiar to you. If not, go back to the introduction on computational graphs. With that in place we can now compute numeric derivatives like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {((x + y)*x): 1, y: 2, x: 7, (x + y): 2})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_gradient(f, {x:2, y:3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see $\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}x}\\Bigr|_{\\substack{x=2\\\\y=3}} = 7$ and $\\frac{\\mathrm{d}f(x,y)}{\\mathrm{d}y}\\Bigr|_{\\substack{x=2\\\\y=3}} = 2$. But the dictionary provides more, also the derivative w.r.t $(x+y)$ is given! Some more examples, to sure the power of this system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(y*y): 1, ((x*x) + (y*y)): 1, y: 6, (x*x): 1, x: 4})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_gradient(x*x+y*y, {x:2, y:3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {y: 125,\n",
       "             (((x + 3)*(y + 4))*z): 5,\n",
       "             z: 350,\n",
       "             (x + 3): 175,\n",
       "             3: 175,\n",
       "             ((x + 3)*(y + 4)): 25,\n",
       "             ((((x + 3)*(y + 4))*z)*z): 1,\n",
       "             4: 125,\n",
       "             (y + 4): 125,\n",
       "             x: 175})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = Symbol('z')\n",
    "numeric_gradient((x+3)*(y+4)*z*z, {x:2, y:3, z:5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing symbolic derivatives\n",
    "\n",
    "Now that we can compute numerica derivates one might wonder if we could do the same symbolically, i.e instead of returning a number we return some expression tree. Clearly such a feature would be beneficial as it would allow computation of higher order derivatives. Additionally, pre-factoring the derivative expressions might be favorable when invoking the derivative evaluation multiple times.\n",
    "\n",
    "Turns out modifying the numeric gradient computation for symbolic computation is straight forward. All that needs to be done is to return appropriate `Node`s instead of numeric values. Infact with the overloaded `+` and `*` operations in place for nodes, the symbolic gradient computation looks nearly identical to `numeric_gradient` as defined earlier.\n",
    "\n",
    "Here it is, `symbolic_gradient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def symbolic_gradient(f):\n",
    "    derivatives = defaultdict(lambda: Constant(0))\n",
    "    \n",
    "    gen = bfs(f, Constant(1))\n",
    "    try:\n",
    "        n, in_grad = next(gen) # Need to use edge info when expressions are reused!\n",
    "        while True:\n",
    "            derivatives[n] = derivatives[n] + in_grad\n",
    "            local_grad = n.symbolic_gradient()\n",
    "            n, in_grad = gen.send([l * in_grad for l in local_grad])\n",
    "    except StopIteration:\n",
    "        return derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only differences: we use `Constant` instead of numbers directly and a method to be defined `symbolic_gradient` is invoked. One can probably guess that the operations `+` and `*` call the corresponding overloads for `Node` objects introduced earlier. As always, let's monkey patch for `symbolic_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Monkey patch for symbolic_gradient\n",
    "Symbol.symbolic_gradient = lambda self: [] # Nothing todo\n",
    "Constant.symbolic_gradient = lambda self: [] # Nothing todo\n",
    "\n",
    "Add.symbolic_gradient = lambda self: [Constant(1), Constant(1)] # dx+y/dx = 1, dx+y/dy = 1\n",
    "Mul.symbolic_gradient = lambda self: [self.children[1], self.children[0]] # dx*y/dx = y, dx*y/dy = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, replace numbers by Constants and for `Mul` just return the opposite child expression. Let's test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.symbolic_gradient.<locals>.<lambda>>,\n",
       "            {((x + y)*x): (0 + 1),\n",
       "             y: (0 + (1*(x*1))),\n",
       "             x: ((0 + ((x + y)*1)) + (1*(x*1))),\n",
       "             (x + y): (0 + (x*1))})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbolic_gradient(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at $x$. It claims derivate is equal to $((0 + ((x + y)*1)) + (1*(x*1)))$. After massaging the terms you indeed find that this is the same as $2x+y$. Although not very readable, the reported results are correct. Something we will tackle in the next section when we begin to simplify expressions. \n",
    "\n",
    "Since the returned dictionary contains expressions, we can apply all our arsenal of functions developed so far onto them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx at (x=2,y=3) is 7\n",
      "df/dy at (x=2,y=3) is 2\n",
      "ddf/dxdx at (x=2,y=3) is 2\n",
      "ddf/dxdy at (x=2,y=3) is 1\n",
      "ddf/dydx at (x=2,y=3) is 1\n",
      "ddf/dydy at (x=2,y=3) is 0\n"
     ]
    }
   ],
   "source": [
    "d = symbolic_gradient(f)\n",
    "print('df/dx at (x=2,y=3) is {}'.format(value(d[x], {x:2, y:3})))\n",
    "print('df/dy at (x=2,y=3) is {}'.format(value(d[y], {x:2, y:3})))\n",
    "\n",
    "# Let's try second order derivatives\n",
    "ddx = symbolic_gradient(d[x])\n",
    "ddy = symbolic_gradient(d[y])\n",
    "print('ddf/dxdx at (x=2,y=3) is {}'.format(value(ddx[x], {x:2, y:3})))\n",
    "print('ddf/dxdy at (x=2,y=3) is {}'.format(value(ddx[y], {x:2, y:3})))\n",
    "print('ddf/dydx at (x=2,y=3) is {}'.format(value(ddy[x], {x:2, y:3})))\n",
    "print('ddf/dydy at (x=2,y=3) is {}'.format(value(ddy[y], {x:2, y:3})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new operations\n",
    "\n",
    "Adding new operations to CGraph is not hard. The following recipe sums up the necessary steps.\n",
    " 1. Add a new class inheriting `Node`\n",
    " 1. Add implementations for `compute_value`, `symbolic_gradient` and optionally for `compute_gradient`\n",
    " 1. Provide one or more free function with prefix `sym_*` that connects arguments as inputs for your operation\n",
    " 1. Optionally provide and implementation for `__str__`\n",
    " 1. Optionally provide new `__*__` methods in `Node` to support improved syntax delegating to `sym_*` methods.\n",
    " \n",
    "Note that `compute_gradient` is optional, as it can always be mimicked by `symbolic_gradient` followed by `value`. \n",
    "\n",
    "A word of caution: when computing numeric gradients through `compute_gradient` you might find yourself in a position of potentially dividing by zero or raising any other math expection. When this happens on path you are even not interested in, gradient computation will stop. For example consider $f(x,y) = x^y$ and let $x=-1, y=2$. Then when evaluating the gradient for $y$ you will find that it corresponds to $x^y*log(x)$. Unfortunately $x$ is negative and so the value can not be computed. Python raises an exception and gradient computation fails.\n",
    "\n",
    "CGraph handles this by using `NAN`s instead of exceptions. Almost all operations that invoke `NAN`s result in `NAN`, so they propagate nicely. In [CGraph](../cgraph.py) a function named `nan_on_fail` is used to catch math exceptions and instead return `NAN`. See`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
